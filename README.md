# OpenAQ Data Engineering Project

This project is a robust pipeline for extracting, processing, and storing air quality data from the [OpenAQ API](https://docs.openaq.org/). It integrates real-time data processing, batch processing, and streaming using cutting-edge technologies like Kafka, Spark, PostgreSQL, and Airflow.

## Key Features
1. **Data Extraction**: Fetches air quality data (parameters, locations, and countries) from the OpenAQ API.
2. **Kafka Streaming**: Utilizes Kafka for streaming data between components.
3. **Batch Processing**: Processes dimension data with Apache Spark.
4. **Real-Time Streaming**: Processes real-time measurements using Spark Streaming.
5. **Data Storage**: Stores processed data in PostgreSQL.
6. **Orchestration and Monitoring**: Uses Airflow to manage tasks and monitor pipelines.

---

## Project Architecture

The pipeline consists of the following steps:
1. **Data Fetching**: Extracts data from the OpenAQ API.
2. **Data Streaming**: Publishes the extracted data to Kafka topics.
3. **Data Processing**: Uses Spark to process and enrich the data.
4. **Data Storage**: Writes processed data to PostgreSQL.
5. **Real-Time Streaming**: Uses Spark Streaming for real-time measurements.

![Project Architecture](img/Project_architecture.jpeg)

---

## Prerequisites

Before running the project, ensure you have:
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

---

## Project Structure

---

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Mehdi-24-K4/DataEngProject.git
cd DataEngProject
```

### 2. Set Up API Key for OpenAQ
Create a file named ```api_key.json``` in the ```dags``` directory to store your OpenAQ API key:
```json
{
  "New_API_KEY": "your_openaq_api_key"
}
```

### 3. Build and Start Docker Services
Run the following command to build and start the services:
```bash
docker-compose up -d --build
```
The following services will be deployed:
- Airflow: For orchestrating the pipeline.
- Kafka: For streaming data between components.
- PostgreSQL: For storing processed data.
- Spark: For processing batch and real-time data.
- Supervisor: For monitoring DAG progress and triggering Spark Streaming.

### 4. Access Airflow UI
- Open your browser and go to ```http://localhost:8080```.
- Log in with the default credentials:
  - Username: ```airflow```
  - Password: ```airflow```.
  
Enable and trigger the DAG named ```initialize_pipeline_dag```.

### 5. Monitor Spark Streaming
Once the DAG completes processing the dimensions, the Supervisor container detects when the last task begins and automatically triggers the Spark Streaming job. This job processes real-time measurements from Kafka and stores them in PostgreSQL.

## ðŸ›  How It Works

### 1. Batch Processing (Airflow DAG)
- Step 1: Extract data from the OpenAQ API using the ```initialize_pipeline_dag.py```.
- Step 2: Process the data with Spark and store it in PostgreSQL via temporary tables.
- Step 3: Kafka topics are used as intermediaries for dimension data (parameters, stations, country).
### 2. Real-Time Processing (Spark Streaming)
- Step 1: Supervisor container detects the final task in the DAG.
- Step 2: Triggers Spark Streaming (```spark_stream.py```) to process the latest measurement data.
- Step 3: Data is stored in PostgreSQL in near real-time.
<!-- 
## Technologies UtilisÃ©es

- **Apache Kafka** : Broker de messages distribuÃ© pour la gestion des flux de donnÃ©es.
- **Zookeeper** : Service de coordination pour Kafka.
- **MongoDB** : Base de donnÃ©es NoSQL utilisÃ©e comme stockage temporaire des donnÃ©es non transformÃ©es.
- **Apache Spark** : Framework de traitement de donnÃ©es en cluster utilisÃ© pour les Ã©tapes de transformation et d'agrÃ©gation des donnÃ©es. Spark permet d'accÃ©lÃ©rer le traitement en distribuant les calculs sur plusieurs partitions.
- **PostgreSQL** : Base de donnÃ©es relationnelle pour le stockage final des donnÃ©es sous un schÃ©ma en Ã©toile, facilitant les requÃªtes analytiques.
- **Airflow** : Orchestrateur de workflows pour la gestion des diffÃ©rentes tÃ¢ches ETL.
- **Docker et Docker Compose** : Pour la conteneurisation et l'isolation des services, facilitant le dÃ©ploiement et la gestion des dÃ©pendances.
- **Grafana** : Pour la visualisation des donnÃ©es collectÃ©es, sous forme de tableaux de bord dynamiques.

## SchÃ©ma en Ã‰toile

Le schÃ©ma en Ã©toile est conÃ§u pour faciliter les analyses OLAP (Online Analytical Processing). Il est composÃ© de :

- **Tables de dimension** :
  - `dimension_location` : stocke les informations de localisation (latitude, longitude).
  - `dimension_parameter` : stocke les paramÃ¨tres surveillÃ©s (comme PM2.5, NO2, etc.).
  - `dimension_time` : stocke les informations temporelles (timestamp).
  
- **Table de faits** :
  - `air_quality_measurements` : contient les mesures de la qualitÃ© de l'air, avec des clÃ©s Ã©trangÃ¨res vers les dimensions `location`, `parameter`, et `time`.

## PrÃ©requis

- [Docker](https://www.docker.com/) et [Docker Compose](https://docs.docker.com/compose/)
- Python 3.x
- AccÃ¨s Ã  l'API OpenAQ (une clÃ© API peut Ãªtre requise selon les conditions d'accÃ¨s)

## Installation

1. **Clonez le dÃ©pÃ´t** :
    ```bash
    git clone https://github.com/username/repository.git
    cd repository
    ```

2. **DÃ©marrez les conteneurs Docker** :
    Utilisez Docker Compose pour dÃ©marrer tous les services nÃ©cessaires (Kafka, Zookeeper, MongoDB, PostgreSQL, Airflow, Spark, etc.) :
    ```bash
    docker-compose up --build
    ```

3. **AccÃ©dez Ã  l'interface Airflow** :
    Une fois les conteneurs dÃ©marrÃ©s, Airflow sera disponible Ã  l'adresse [http://localhost:8084](http://localhost:8084). Vous pouvez y gÃ©rer et surveiller l'exÃ©cution du pipeline.

## Utilisation

### 1. Extraction des donnÃ©es
Le pipeline ETL est orchestrÃ© avec Apache Airflow. Le DAG `air_quality_pipeline` est configurÃ© pour s'exÃ©cuter toutes les 10 minutes, ce qui signifie qu'il collectera automatiquement de nouvelles donnÃ©es depuis l'API OpenAQ, les enverra Ã  Kafka, puis les transformera et les chargera dans PostgreSQL.

### 2. Orchestration des tÃ¢ches
Le pipeline est composÃ© de trois tÃ¢ches principales :
- **Extraction** : `produce_air_quality_data` â€” Cette tÃ¢che interagit avec l'API OpenAQ pour extraire les donnÃ©es et les envoyer Ã  Kafka.
- **Consommation** : `consume_and_store_data` â€” Les donnÃ©es extraites sont consommÃ©es par Kafka et stockÃ©es dans MongoDB.
- **Transformation et Chargement** : `transform_store_postgreSQL` â€” Ã€ l'aide de Spark, les donnÃ©es sont nettoyÃ©es, les tables de dimensions et de faits sont crÃ©Ã©es, et les donnÃ©es sont chargÃ©es dans PostgreSQL.

Vous pouvez surveiller et gÃ©rer ces tÃ¢ches via l'interface Airflow.

### 3. Visualisation des donnÃ©es
Une fois les donnÃ©es chargÃ©es dans PostgreSQL, vous pouvez les visualiser et les analyser avec Grafana. Un tableau de bord Grafana peut Ãªtre configurÃ© pour suivre les tendances des mesures de qualitÃ© de l'air au fil du temps.

## Configuration du DAG dans Airflow

Le DAG `air_quality_pipeline` se trouve dans le fichier `dags/air_quality_pipeline.py`. Il est configurÃ© pour s'exÃ©cuter toutes les 10 minutes et suit cette sÃ©quence :

1. Extraction des donnÃ©es depuis OpenAQ avec la tÃ¢che `produce_air_quality_data`.
2. Consommation des donnÃ©es Kafka avec `consume_and_store_data` et stockage temporaire dans MongoDB.
3. Transformation des donnÃ©es et chargement dans PostgreSQL avec `transform_store_postgreSQL`.

## Optimisation avec Apache Spark

Le traitement des donnÃ©es dans ce pipeline est optimisÃ© grÃ¢ce Ã  Apache Spark :
- Spark permet de gÃ©rer efficacement des volumes importants de donnÃ©es en distribuant les calculs sur plusieurs partitions.
- Les donnÃ©es sont d'abord transformÃ©es dans des tables de dimensions et de faits. Les dimensions `location`, `parameter`, et `time` sont dÃ©dupliquÃ©es et attribuÃ©es des ID uniques.
- Le pipeline utilise Spark SQL pour appliquer les transformations complexes et les jointures nÃ©cessaires.
- La coalescence des partitions dans Spark permet de rÃ©duire le nombre de partitions avant le chargement dans PostgreSQL, optimisant ainsi l'insertion des donnÃ©es.

## Surveillance et Debugging

- **Airflow** : Airflow permet de suivre le statut des tÃ¢ches en temps rÃ©el et de voir les logs dÃ©taillÃ©s en cas d'erreurs.
- **Logs Spark** : Les logs des tÃ¢ches Spark peuvent Ãªtre consultÃ©s via les journaux Airflow ou directement dans le conteneur Spark.
- **Base de donnÃ©es PostgreSQL** : Vous pouvez accÃ©der Ã  la base PostgreSQL pour vÃ©rifier que les donnÃ©es ont bien Ã©tÃ© chargÃ©es.

## Conclusion

Ce projet met en place un pipeline ETL robuste et scalable pour collecter, transformer et analyser les donnÃ©es de la qualitÃ© de l'air. GrÃ¢ce Ã  des outils comme Kafka, Spark, et PostgreSQL, il est capable de gÃ©rer efficacement des volumes importants de donnÃ©es tout en offrant des possibilitÃ©s d'analyse via un schÃ©ma en Ã©toile.

---

## Auteur

*Ouazzani Jamil Mehdi* - [Votre profil GitHub](https://github.com/Mehdi-24-K4) -->